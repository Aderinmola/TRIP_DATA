{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/opt/spark'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SPARK_HOME = os.environ.get('SPARK_HOME')\n",
    "SPARK_HOME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "findspark.init(spark_home=SPARK_HOME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "24/03/06 14:58:35 WARN Utils: Your hostname, DESKTOP-TINF367 resolves to a loopback address: 127.0.1.1; using 172.28.17.111 instead (on interface eth0)\n",
      "24/03/06 14:58:35 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/03/06 14:58:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "|dispatching_base_num|    pickup_datetime|   dropOff_datetime|PUlocationID|DOlocationID|SR_Flag|Affiliated_base_number|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "|              B00009|2019-10-01 00:23:00|2019-10-01 00:35:00|         264|         264|   null|                B00009|\n",
      "|              B00013|2019-10-01 00:11:29|2019-10-01 00:13:22|         264|         264|   null|                B00013|\n",
      "|              B00014|2019-10-01 00:11:43|2019-10-01 00:37:20|         264|         264|   null|                B00014|\n",
      "|              B00014|2019-10-01 00:56:29|2019-10-01 00:57:47|         264|         264|   null|                B00014|\n",
      "|              B00014|2019-10-01 00:23:09|2019-10-01 00:28:27|         264|         264|   null|                B00014|\n",
      "|     B00021         |2019-10-01 00:00:48|2019-10-01 00:07:12|         129|         129|   null|       B00021         |\n",
      "|     B00021         |2019-10-01 00:47:23|2019-10-01 00:53:25|          57|          57|   null|       B00021         |\n",
      "|     B00021         |2019-10-01 00:10:06|2019-10-01 00:19:50|         173|         173|   null|       B00021         |\n",
      "|     B00021         |2019-10-01 00:51:37|2019-10-01 01:06:14|         226|         226|   null|       B00021         |\n",
      "|     B00021         |2019-10-01 00:28:23|2019-10-01 00:34:33|          56|          56|   null|       B00021         |\n",
      "|     B00021         |2019-10-01 00:31:17|2019-10-01 00:51:52|          82|          82|   null|       B00021         |\n",
      "|              B00037|2019-10-01 00:07:41|2019-10-01 00:15:23|         264|          71|   null|                B00037|\n",
      "|              B00037|2019-10-01 00:13:38|2019-10-01 00:25:51|         264|          39|   null|                B00037|\n",
      "|              B00037|2019-10-01 00:42:40|2019-10-01 00:53:47|         264|         188|   null|                B00037|\n",
      "|              B00037|2019-10-01 00:58:46|2019-10-01 01:10:11|         264|          91|   null|                B00037|\n",
      "|              B00037|2019-10-01 00:09:49|2019-10-01 00:14:37|         264|          71|   null|                B00037|\n",
      "|              B00037|2019-10-01 00:22:35|2019-10-01 00:36:53|         264|          35|   null|                B00037|\n",
      "|              B00037|2019-10-01 00:54:27|2019-10-01 01:03:37|         264|          61|   null|                B00037|\n",
      "|              B00037|2019-10-01 00:08:12|2019-10-01 00:28:47|         264|         198|   null|                B00037|\n",
      "|              B00053|2019-10-01 00:05:24|2019-10-01 00:53:03|         264|         264|   null|                  #N/A|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read zipped file directly from Spark\n",
    "df_zipped = spark \\\n",
    "    .read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", True) \\\n",
    "    .load(\"fhv_tripdata_2019-10.csv\")\n",
    "df_zipped.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 1001 fhv_tripdata_2019-10.csv > headlinall.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dispatching_base_num       object\n",
       "pickup_datetime            object\n",
       "dropOff_datetime           object\n",
       "PUlocationID              float64\n",
       "DOlocationID              float64\n",
       "SR_Flag                   float64\n",
       "Affiliated_base_number     object\n",
       "dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_df = pd.read_csv('headlinall.csv')\n",
    "\n",
    "second_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('dispatching_base_num', StringType(), True), StructField('pickup_datetime', StringType(), True), StructField('dropOff_datetime', StringType(), True), StructField('PUlocationID', DoubleType(), True), StructField('DOlocationID', DoubleType(), True), StructField('SR_Flag', DoubleType(), True), StructField('Affiliated_base_number', StringType(), True)])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.createDataFrame(second_df).schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = types.StructType([\n",
    "    types.StructField('dispatching_base_num', types.StringType(), True), \n",
    "    types.StructField('pickup_datetime', types.TimestampType(), True), \n",
    "    types.StructField('dropOff_datetime', types.TimestampType(), True), \n",
    "    types.StructField('PUlocationID', types.IntegerType(), True), \n",
    "    types.StructField('DOlocationID', types.IntegerType(), True), \n",
    "    types.StructField('SR_Flag', types.StringType(), True), \n",
    "    types.StructField('Affiliated_base_number', types.StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(schema) \\\n",
    "    .csv(\"fhv_tripdata_2019-10.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# partition\n",
    "df = df.repartition(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# write to parquet\n",
    "\n",
    "df.write.parquet('fhv/2019/10/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropOff_datetime: timestamp (nullable = true)\n",
      " |-- PUlocationID: integer (nullable = true)\n",
      " |-- DOlocationID: integer (nullable = true)\n",
      " |-- SR_Flag: string (nullable = true)\n",
      " |-- Affiliated_base_number: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# How many taxi trips were there on the 15th of October?\n",
    "\n",
    "# spark dataframes\n",
    "df = spark.read.parquet('fhv/2019/10/')\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "62610"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fifteenth_of_october = df.select('dispatching_base_num', 'pickup_datetime', 'dropoff_datetime', 'PULocationID', 'DOLocationID') \\\n",
    "    .filter((df.pickup_datetime >= '2019-10-15') & (df.pickup_datetime < '2019-10-16')) \\\n",
    "    .count()\n",
    "df_fifteenth_of_october"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/spark/python/pyspark/sql/dataframe.py:330: FutureWarning: Deprecated in 2.0, use createOrReplaceTempView instead.\n",
      "  warnings.warn(\"Deprecated in 2.0, use createOrReplaceTempView instead.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Longest trip for each day\n",
    "\n",
    "# What is the length of the longest trip in the dataset in hours?\n",
    "\n",
    "# Get started by creating temp tables <====>\n",
    "df.registerTempTable(\"trips_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17:======================================>                   (4 + 2) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+------------+------------+----------+\n",
      "|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|difference|\n",
      "+-------------------+-------------------+------------+------------+----------+\n",
      "|2019-10-28 09:00:00|2091-10-28 09:30:00|         264|         264|    631152|\n",
      "|2019-10-11 18:00:00|2091-10-11 18:30:00|         264|         264|    631152|\n",
      "|2019-10-31 23:46:33|2029-11-01 00:13:00|        null|        null|     87672|\n",
      "|2019-10-01 21:43:42|2027-10-01 21:45:23|         159|         264|     70128|\n",
      "|2019-10-17 14:00:00|2020-10-18 00:00:00|        null|        null|      8794|\n",
      "|2019-10-26 21:26:00|2020-10-26 21:36:00|         264|         264|      8784|\n",
      "|2019-10-30 12:30:04|2019-12-30 13:02:08|         264|          50|      1464|\n",
      "|2019-10-25 07:04:57|2019-12-08 07:21:11|         168|         235|      1056|\n",
      "|2019-10-25 07:04:57|2019-12-08 07:54:33|         168|         235|      1056|\n",
      "|2019-10-01 07:21:12|2019-11-03 08:44:21|           5|           6|       793|\n",
      "|2019-10-01 13:47:17|2019-11-03 15:20:28|          44|         214|       793|\n",
      "|2019-10-01 13:41:00|2019-11-03 14:58:51|         206|         172|       793|\n",
      "|2019-10-01 07:48:15|2019-11-03 08:06:37|         204|         109|       792|\n",
      "|2019-10-01 14:18:49|2019-11-03 14:51:25|          84|         118|       792|\n",
      "|2019-10-01 11:41:57|2019-11-03 11:53:31|           6|         251|       792|\n",
      "|2019-10-01 15:40:40|2019-11-03 15:55:58|           6|         172|       792|\n",
      "|2019-10-01 13:10:24|2019-11-03 13:15:29|         206|         206|       792|\n",
      "|2019-10-01 18:43:20|2019-11-03 19:43:13|          23|           5|       792|\n",
      "|2019-10-01 09:19:02|2019-11-03 09:29:12|         118|         176|       792|\n",
      "|2019-10-01 13:09:14|2019-11-03 13:23:21|         118|          44|       792|\n",
      "+-------------------+-------------------+------------+------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    pickup_datetime,\n",
    "    dropoff_datetime,\n",
    "    PULocationID,\n",
    "    DOLocationID,\n",
    "    TIMESTAMPDIFF(HOUR, pickup_datetime, dropoff_datetime) AS difference\n",
    "\n",
    "FROM\n",
    "    trips_data\n",
    "ORDER BY\n",
    "    difference DESC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+--------------------+------------+\n",
      "|LocationID|      Borough|                Zone|service_zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "|         1|          EWR|      Newark Airport|         EWR|\n",
      "|         2|       Queens|         Jamaica Bay|   Boro Zone|\n",
      "|         3|        Bronx|Allerton/Pelham G...|   Boro Zone|\n",
      "|         4|    Manhattan|       Alphabet City| Yellow Zone|\n",
      "|         5|Staten Island|       Arden Heights|   Boro Zone|\n",
      "|         6|Staten Island|Arrochar/Fort Wad...|   Boro Zone|\n",
      "|         7|       Queens|             Astoria|   Boro Zone|\n",
      "|         8|       Queens|        Astoria Park|   Boro Zone|\n",
      "|         9|       Queens|          Auburndale|   Boro Zone|\n",
      "|        10|       Queens|        Baisley Park|   Boro Zone|\n",
      "|        11|     Brooklyn|          Bath Beach|   Boro Zone|\n",
      "|        12|    Manhattan|        Battery Park| Yellow Zone|\n",
      "|        13|    Manhattan|   Battery Park City| Yellow Zone|\n",
      "|        14|     Brooklyn|           Bay Ridge|   Boro Zone|\n",
      "|        15|       Queens|Bay Terrace/Fort ...|   Boro Zone|\n",
      "|        16|       Queens|             Bayside|   Boro Zone|\n",
      "|        17|     Brooklyn|             Bedford|   Boro Zone|\n",
      "|        18|        Bronx|        Bedford Park|   Boro Zone|\n",
      "|        19|       Queens|           Bellerose|   Boro Zone|\n",
      "|        20|        Bronx|             Belmont|   Boro Zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Least frequent pickup location zone\n",
    "\n",
    "# Load the zone lookup data into a temp view in Spark\n",
    "\n",
    "zone_df = spark.read.option(\"header\", \"true\").csv('taxi_zone_lookup.csv')\n",
    "\n",
    "zone_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zone_df.registerTempTable(\"zone_data\")\n",
    "\n",
    "# join zone_df to main df\n",
    "\n",
    "df_join = df.join(zone_df, df.PUlocationID == zone_df.LocationID, 'inner')\n",
    "\n",
    "# zone_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/spark/python/pyspark/sql/dataframe.py:330: FutureWarning: Deprecated in 2.0, use createOrReplaceTempView instead.\n",
      "  warnings.warn(\"Deprecated in 2.0, use createOrReplaceTempView instead.\", FutureWarning)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+----------+---------+--------------------+------------+\n",
      "|dispatching_base_num|    pickup_datetime|   dropOff_datetime|PUlocationID|DOlocationID|SR_Flag|Affiliated_base_number|LocationID|  Borough|                Zone|service_zone|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+----------+---------+--------------------+------------+\n",
      "|              B00445|2019-10-07 03:54:41|2019-10-07 04:02:41|         252|         138|   null|                B00445|       252|   Queens|          Whitestone|   Boro Zone|\n",
      "|              B02613|2019-10-05 00:47:05|2019-10-05 01:11:53|         264|          39|   null|                B02613|       264|  Unknown|                  NV|         N/A|\n",
      "|              B01437|2019-10-02 20:29:31|2019-10-02 20:52:31|         264|         173|   null|                B01437|       264|  Unknown|                  NV|         N/A|\n",
      "|              B02292|2019-10-02 16:40:52|2019-10-02 16:53:57|         264|          17|   null|                B02292|       264|  Unknown|                  NV|         N/A|\n",
      "|              B01338|2019-10-03 17:21:08|2019-10-03 17:26:58|         264|          32|   null|                B01338|       264|  Unknown|                  NV|         N/A|\n",
      "|              B02715|2019-10-08 13:10:21|2019-10-08 13:27:26|         236|         233|   null|                B02788|       236|Manhattan|Upper East Side N...| Yellow Zone|\n",
      "|              B03060|2019-10-04 17:23:21|2019-10-04 17:46:10|         264|          22|   null|                B03060|       264|  Unknown|                  NV|         N/A|\n",
      "|              B02285|2019-10-02 19:55:00|2019-10-02 21:07:00|         264|         264|   null|                B02285|       264|  Unknown|                  NV|         N/A|\n",
      "|              B02311|2019-10-06 02:03:52|2019-10-06 02:17:22|         264|          89|   null|                B02311|       264|  Unknown|                  NV|         N/A|\n",
      "|              B01315|2019-10-08 13:18:59|2019-10-08 13:35:23|         264|          78|   null|                B01315|       264|  Unknown|                  NV|         N/A|\n",
      "|              B02285|2019-10-01 20:08:11|2019-10-01 21:01:00|         264|         264|   null|                B02285|       264|  Unknown|                  NV|         N/A|\n",
      "|              B02293|2019-10-02 10:25:20|2019-10-02 12:49:37|          74|          61|   null|                B02293|        74|Manhattan|   East Harlem North|   Boro Zone|\n",
      "|              B01087|2019-10-01 08:52:12|2019-10-01 16:00:14|          43|          43|   null|                B01087|        43|Manhattan|        Central Park| Yellow Zone|\n",
      "|              B03160|2019-10-05 06:19:00|2019-10-05 06:31:00|          60|         184|   null|                B02133|        60|    Bronx|   Crotona Park East|   Boro Zone|\n",
      "|              B01087|2019-10-03 22:14:32|2019-10-03 22:43:34|         142|         235|   null|                B01087|       142|Manhattan| Lincoln Square East| Yellow Zone|\n",
      "|              B00256|2019-10-02 21:25:36|2019-10-02 21:58:51|         264|         264|   null|                B00256|       264|  Unknown|                  NV|         N/A|\n",
      "|              B00900|2019-10-03 11:57:10|2019-10-03 12:02:54|         264|         197|   null|                B00900|       264|  Unknown|                  NV|         N/A|\n",
      "|              B02905|2019-10-02 09:24:09|2019-10-02 09:30:10|         264|         228|   null|                B02905|       264|  Unknown|                  NV|         N/A|\n",
      "|              B00821|2019-10-07 04:01:35|2019-10-07 04:07:42|         264|          77|   null|                B00821|       264|  Unknown|                  NV|         N/A|\n",
      "|              B02838|2019-10-05 19:00:36|2019-10-05 19:10:26|         264|         133|   null|                B02838|       264|  Unknown|                  NV|         N/A|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+----------+---------+--------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create table from the joined table\n",
    "\n",
    "df_join.registerTempTable(\"join_data\")\n",
    "df_join.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 26:================================================>         (5 + 1) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+\n",
      "|                Zone|Zone_Frequency|\n",
      "+--------------------+--------------+\n",
      "|         Jamaica Bay|             1|\n",
      "|Governor's Island...|             2|\n",
      "| Green-Wood Cemetery|             5|\n",
      "|       Broad Channel|             8|\n",
      "|     Highbridge Park|            14|\n",
      "|        Battery Park|            15|\n",
      "|Saint Michaels Ce...|            23|\n",
      "|Breezy Point/Fort...|            25|\n",
      "|Marine Park/Floyd...|            26|\n",
      "|        Astoria Park|            29|\n",
      "|    Inwood Hill Park|            39|\n",
      "|       Willets Point|            47|\n",
      "|Forest Park/Highl...|            53|\n",
      "|  Brooklyn Navy Yard|            57|\n",
      "|        Crotona Park|            62|\n",
      "|        Country Club|            77|\n",
      "|     Freshkills Park|            89|\n",
      "|       Prospect Park|            98|\n",
      "|     Columbia Street|           105|\n",
      "|  South Williamsburg|           110|\n",
      "+--------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    Zone,\n",
    "    COUNT(Zone) AS Zone_Frequency\n",
    "FROM\n",
    "    join_data\n",
    "GROUP BY Zone\n",
    "ORDER BY Zone_Frequency\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
